---
title: "Movie Lens Project"
author: "F. Seka"
date: "14/05/2019"
output: pdf_document
toc: true
toc_depth: 4
number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The purpose of this report is to document the various strategies investigated in the frame of the Movie Lens final project, in partial fulfillment of the requirements for the EDX Harvard Data Science Professional Certificate.

In the first part of the report, information is briefly given about the structuring and preparation of the data used to build and evaluate the machine learning algorithms. The second part describes in depth each approach analysed as part of this project. A final, optimized, algorithm is selected and tested agains the test dataset.

## Data preparation

**Note:** the data preparation script provided in the project instructions creates the edx end validation datasets. For the sake of efficiency, these datasets have been saved locally in R.Data format, hence avoiding a download of the data at each R session.

The data was structured using following script

```{r preparation, eval=T, message=F, warning=F, results='hide'}
library(tidyverse)
library(caret)

load("DataReload.RData")
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)

edx_train <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in validation set are also in edx set

edx_test <- temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, edx_test)
edx_train <- rbind(edx_train, removed)

rm(temp, removed)


```

Resulting in the following structure:

* edx
    + edx_train: subset of the edx dataset used for training
    + edx_test: subset of the edx dataset used for performance/cross-validation
* validation: data set used as test set for the final evaluation


## Analysis

### Root Mean Square Error Calculation

As a reminder, the Root Mean Squere Error Calculation (which we will call RMSE from now on) will be used to evaluate how close the predictions are to the true values in the *validation* set.

```{r rmse_function, eval=T}
RMSE <- function(true_ratings, predicted_ratings){
  
  sqrt(mean((true_ratings - predicted_ratings)^2))

  }
```

### Naive prediction

In a first approach, a simple strategy is to predict the rating of a movie using the rating mean value from the training dataset. This approach does not consider the specificity of the user.

$$Y_{u,i}=\mu+\epsilon_{u,i}$$

```{r naive_approach, message=F, warning=F}

# Calculate the mean of the ratings
mu_hat <- mean(edx_train$rating)

# Use the RMSE function created to evaluate the error.
# As expected, this strategy will not provide the best RMSE.

simple_approach_rmse<-RMSE(validation$rating, mu_hat)

# We will create a dataframe to store our different results, for comparison purposes.
rmse_results <- data_frame(method = "Naive Mean approach", RMSE = simple_approach_rmse)

```

This resulting in the following RMSE

```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```

### Movie effect model

As learned from the course, we will attempt to model the movie effect, where some movies simply get rated better than others. This can be modelled through a bias term.

$$Y_{u,i}=\mu+b_i+\epsilon_{u,i}$$

The implementation is as follows:


```{r message=F, warning=F}
mu <- mean(edx_train$rating) 
movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
# If we now test the performance of this approach
predicted_ratings <- edx_test %>% mutate(rating=mu) %>%
  left_join(movie_avgs, by='movieId') %>% mutate(prediction=rating+b_i)
movie_effect_result<- RMSE(edx_test$rating,predicted_ratings$prediction)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = movie_effect_result))
```

This resulting in the following RMSE

```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```


As we can see, implementing the movie effect already brings some improvement to predictions.

### Movie and user model


$$Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i}$$
The implementation is as follows:


```{r message=F, warning=F}

user_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- edx_test %>% 
  mutate(rating=mu) %>% 
  left_join(movie_avgs, by='movieId') %>% 
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

user_effect_result<- RMSE(edx_test$rating,predicted_ratings)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and User Effect Model",  
                                     RMSE = user_effect_result))
```

This results in the following RMSE: 



```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```


### Regularization - Movie Effect

As seen in the course, noise can be introduced in data when a small portion of sample can cause large estimates, hence providing misleading results.

An easy way to see this effect is to look at the best and worst rated movies:

```{r echo=FALSE}
movie_titles <- edx_train %>% 
  select(movieId, title) %>%
  distinct()
```

Worst rated movies in the training data set:
```{r message=F}
edx_train %>% count(movieId) %>% 
  left_join(movie_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10)
```

Best rated movies in the training data set:
```{r message=F}
edx_train %>% count(movieId) %>% 
  left_join(movie_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10)
```
All these movies appear to be rather singular, which is confirmed by their count of reviews.

The following code explores the possibility to introduce a penalization factor such that movies that have not been rated by a significant number of reviewers weighs less than movies that have been largely rated. We test different values of the factor and identify the one minimizing the RMSE (details provided in the dedicated subsection below)

The penalized least square terms are defined by the equation

$$\hat{b}_i(\lambda)=\frac{1}{\lambda+n_i}\sum^{n_i}_{u=1}(Y_{u,i}-\hat{\mu})$$

```{r message=F, warning=F}
lambda <- 1.5 # this value of lambda was determined through iterative tests, 
# between 0 and 10, in 0.25 increments. 

mu <- mean(edx_train$rating)
movie_reg_avgs <- edx_train %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n())
predicted_ratings <- edx_test %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
model_3_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse))
```

It turns out that the results provided through this method slightly improve the Movie effect Model results, but not to a significant degree.

Looking at the best and worst rated films taking into account the penalization factor, we can see that the films listed are not these obscure films anymore:

Worst rated movies in the training data set with penalized estimates:
```{r message=F}
edx_train %>% count(movieId) %>% 
  left_join(movie_reg_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10)
```
Best rated movies in the training data set with penalized estimates:
```{r message=F}
edx_train %>% count(movieId) %>% 
  left_join(movie_reg_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(desc(b_i)) %>%
    select(title, b_i, n) %>% 
  slice(1:10)
```
These ranks appear to make more sense now, with reknowned films being listed.

#### Determination of the regularization lambda - Movie effect Model

The determination of the optimal lambda (over the training data set) was carried out using the following code:

```{r message=F, warning=F}
lambdas <- seq(0, 10, 0.25)

mu <- mean(edx_train$rating)
just_the_sum <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- edx_test %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test$rating))
})
```
Ploting the RMSE as a function of the lambdas provides

```{r message=F, warning=F}
qplot(lambdas, rmses) 
```

### Regularization - Movie and User Effect

With the same approach, but based indluding the user effect, a regularized model can be built, with an adequate penalization factor lambda.

In this case, the user terms can be expressed as (the movie least squere terms keep their previous expression):

$$\hat{b}_u(\lambda)=\frac{1}{\lambda+n_u}\sum^{n_u}_{u=1}(Y_{u,i}-b_i-\hat{\mu})$$


#### Determination of the regularization lambda - Movie and user effect Model

The determination of the optimal lambda (over the training data set) was carried out using the following code:

```{r message=F, warning=F}
lambdas <- seq(0, 10, 1)

rmses2 <- sapply(lambdas, function(l){

  mu <- mean(edx_train$rating)
  
  b_i <- edx_train %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx_train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    edx_test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, edx_test$rating))
})
```
Ploting the RMSE as a function of the lambdas provides

```{r message=F, warning=F}
qplot(lambdas, rmses2) 
```

The best value for the regularized movie and user effect model is therefore 5.

#### Regularized movie and user effect model RMSE
```{r message=F, warning=F}
lambda2 <- 5

mu <- mean(edx_train$rating)
b_i <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda2))
b_u <- edx_train %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda2))
predicted_ratings <- 
  edx_test %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_4_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie and User Effect Model",  
                                     RMSE = model_4_rmse))
```

At this stage, the summary of the achived RMSE achieved over the training dataset is as follows:

This resulting in the following RMSE

```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```

We can see that the regularisation has in each case (movie effect and movie/user effect models), provided a slight improvement of the RMSE.


### Final choice and evaluation on the validation dataset


Based on the results outlined before, the retained approach is the **regularised movie and user effect model**.
We will now test this approach on the validation dataset:


```{r message=F, warning=F}
lambda2 <- 5

mu <- mean(edx_train$rating)
b_i <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda2))
b_u <- edx_train %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda2))
predicted_ratings <- 
  validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_final_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
data_frame(method="Regularized Movie and User Effect Model - Validation Set",  
                                     RMSE = model_final_rmse))
```


The final RMSE obtained over the validation dataset is  `r model_final_rmse` .



```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```

# Conclusion

Using a regularised model taking into account the user and movie effects, a final RMSE of `r model_final_rmse` over the validation data set could be achieved. Further improvement could be considered, such as the genre effect, given the possibility for some film genres to be generally more popular than others (e.g. comedy or action). Another direction to explore would be to introduce into the model the personal preference of each user as a contributing factor to the prediction.

# References

* EDX Harvard Data Science Course
* Data Analysis and Prediction Algorithms with R

