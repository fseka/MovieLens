---
title: "Movie Lens Project"
author: "F. Seka"
date: "05/04/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The purpose of this report is to document the various strategies investigated in the frame of the Movie Lens final project, in partial fulfillment of the requirements for the EDX Harvard Data Science Professional Certificate.

In the first part of the report, information is briefly given about the structuring and preparation of the data used to build and evaluate the machine learning algorithms. The second part describes in depth each approach analysed as part of this project. A final, optimized, algorithm is selected and tested agains the test dataset.

## Data preparation

**Note:** the data preparation script provided in the project instructions creates the edx end validation datasets. For the sake of efficiency, these datasets have been saved locally in R.Data format, hence avoiding a download of the data at each R session.

The data was structured using following script

```{r preparation, eval=T, message=F, warning=F, results='hide'}
library(tidyverse)
library(caret)

load("DataReload.RData")
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)

edx_train <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in validation set are also in edx set

edx_test <- temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, edx_test)
edx_train <- rbind(edx_train, removed)

rm(temp, removed)


```

Resulting in the following structure:

* edx
    + edx_train: subset of the edx dataset used for training
    + edx_test: subset of the edx dataset used for performance/cross-validation
* validation: data set used as test set for the final evaluation


## Analysis

### Root Mean Square Error Calculation

As a reminder, the Root Mean Squere Error Calculation (which we will call RMSE from now on) will be used to evaluate how close the predictions are to the true values in the *validation* set.

```{r rmse_function, eval=T}
RMSE <- function(true_ratings, predicted_ratings){
  
  sqrt(mean((true_ratings - predicted_ratings)^2))

  }
```

### Naive prediction

In a first approach, a simple strategy is to predict the rating of a movie using the rating mean value from the training dataset. This approach does not consider the specificity of the user.

$$Y_{u,i}=\mu+\epsilon_{u,i}$$

```{r naive_approach, message=F, warning=F}

# Calculate the mean of the ratings
mu_hat <- mean(edx_train$rating)

# Use the RMSE function created to evaluate the error. As expected, this strategy will not provide the best RMSE.
simple_approach_rmse<-RMSE(validation$rating, mu_hat)

# We will create a dataframe to store our different results, for comparison purposes.
rmse_results <- data_frame(method = "Naive Mean approach", RMSE = simple_approach_rmse)

```

This resulting in the following RMSE

```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```

### Movie effect model

As learned from the course, we will attempt to model the movie effect, where some movies simply get rated better than others. This can be modelled through a bias term.

$$Y_{u,i}=\mu+b_i+\epsilon_{u,i}$$

The implementation is as follows:


```{r message=F, warning=F}
mu <- mean(edx_train$rating) 
movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
# If we now test the performance of this approach
predicted_ratings <- edx_test %>% mutate(rating=mu) %>% left_join(movie_avgs, by='movieId') %>% mutate(prediction=rating+b_i)
movie_effect_result<- RMSE(edx_test$rating,predicted_ratings$prediction)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = movie_effect_result))
```

This resulting in the following RMSE

```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```


As we can see, implementing the movie effect already brings some improvement to predictions.

### Movie and user model


$$Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i}$$
The implementation is as follows:


```{r message=F, warning=F}

user_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- edx_test %>% 
  mutate(rating=mu) %>% 
  left_join(movie_avgs, by='movieId') %>% 
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

user_effect_result<- RMSE(edx_test$rating,predicted_ratings)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and User Effect Model",  
                                     RMSE = user_effect_result))
```
This resulting in the following RMSE

```{r echo=FALSE}
library(knitr)
kable(rmse_results)
```


### Regularization - Movie Effect

As seen in the course, noise can be introduced in data when a small portion of sample ccan cause large estimates, hence providing misleading results.

An easy way to see this effect is to look at the best and worst rated movies:

```{r echo=FALSE}
movie_titles <- edx_train %>% 
  select(movieId, title) %>%
  distinct()
```

Best rated movies in the training data set:
```{r message=F}
edx_train %>% count(movieId) %>% 
  left_join(movie_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10)
```

Worst rated movies in the training data set:
```{r message=F}
edx_train %>% count(movieId) %>% 
  left_join(movie_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10)
```
All these movies appear to be rather singular, which is confirmed by their count of reviews.



The following code explores the possibility to introduce a penalization factor such that movies that have not been rated by a significant number of reviewers weighs less than movies that have been largely rated. We test different values of the factor and identify the one minimizing the RMSE (details provided in the dedicated subsection below).

```{r message=F, warning=F}
lambda <- 1.5 # this value of lambda was determined through iterative tests, between 0 and 10, in 0.25 increments. 

mu <- mean(edx_train$rating)
movie_reg_avgs <- edx_train %>% group_by(movieId) %>% summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n())
predicted_ratings <- edx_test %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
model_3_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse))
```

It turns out that the results provided through this method slightly improve the Movie effect Model results, but not to a significant degree.

#### Determination of the regularization lambda

The determination of the optimal lambda (over the training data set) was carried out using the following code:

```{r message=F, warning=F}
lambdas <- seq(0, 10, 0.25)

mu <- mean(edx_train$rating)
just_the_sum <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- edx_test %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test$rating))
})
```
Ploting the RMSE as a function of the lambdas provides

```{r message=F, warning=F}
qplot(lambdas, rmses) 
```

### Regularization - Movie and User Effect

With the same approach, but based indluding the user effect, a regularized model can be built, with an adequate penalization factor lambda:

### Final choice and evaluation on the test dataset

# Conclusion

## Further possible explotations
### Investigation of the movie genre impact

# References

This is a test